<!DOCTYPE html><html lang="zh-CN" class="h-full"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Cache Oblivious GEMM Implementation · L1 Cached Papers</title><meta name="description" content="Only the Papers That Matter — HPC, LLM Inference, Crypto"><link rel="icon" href="/favicon.svg"><script defer src="https://code.iconify.design/iconify-icon/2.0.0/iconify-icon.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><link rel="stylesheet" href="/_astro/_slug_.BmZYnI04.css"></head> <body class="min-h-screen bg-white text-slate-900"> <div aria-hidden class="h-0.5 bg-gradient-to-r from-teal-500 via-amber-400 to-teal-500"></div> <header class="border-b bg-white/80 backdrop-blur"> <div class="mx-auto max-w-5xl px-4 py-4 flex items-center justify-between"> <a href="/" class="font-semibold">L1 Cached Papers</a> <nav class="flex items-center gap-4 text-sm"> <a href="/archive/" class="hover:underline">Cacheline(Archive)</a> <a href="/about/" class="hover:underline">About</a> </nav> </div> </header> <!-- <main class="mx-auto max-w-5xl px-4 py-8"> --> <main class="mx-auto max-w-6xl px-4 py-8"></main> <button type="button" aria-label="Open Contents" data-toc-toggle aria-expanded="false" class="fixed left-4 bottom-6 z-50 inline-flex items-center justify-center
             size-10 rounded-full border bg-white/90 backdrop-blur shadow hover:bg-white
             text-slate-700 focus:outline-none focus:ring-2 focus:ring-slate-300"><svg xmlns="http://www.w3.org/2000/svg" data-toc-icon class="size-5 transition-transform duration-200" viewBox="0 0 24 24" fill="none" stroke="currentColor"><!-- 默认右箭头；打开时我们给 svg 加 rotate-180 → 变左箭头 --><path d="M9 6l6 6-6 6" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="sr-only">Contents</span></button><div data-toc-overlay class="hidden fixed inset-0 z-40 bg-black/20"></div><aside data-toc-drawer class="fixed left-0 top-0 z-40 h-screen w-72 -translate-x-full transition-transform duration-200
             bg-white border-r shadow-xl overflow-y-auto"><div class="px-4 py-3 border-b flex items-center justify-between"><div class="text-xs uppercase tracking-wider text-slate-500">Contents</div><button type="button" class="text-slate-500 hover:text-slate-700" onclick="document.querySelector('[data-toc-overlay]').click()">✕</button></div><nav class="p-3"><ul class="space-y-1"><li class="pl-0"><a href="#pre-requirement" class="block truncate text-sm text-slate-600 hover:text-slate-900">Pre-requirement</a></li><li class="pl-0"><a href="#cache-oblivious-gemm-implementation-report" class="block truncate text-sm text-slate-600 hover:text-slate-900">Cache-oblivious GEMM Implementation Report</a></li><li class="pl-4"><a href="#v1-naive-gemm" class="block truncate text-sm text-slate-600 hover:text-slate-900">v1: naive GEMM</a></li><li class="pl-4"><a href="#v2-strategy-multi-threading" class="block truncate text-sm text-slate-600 hover:text-slate-900">v2: Strategy: Multi-Threading</a></li><li class="pl-4"><a href="#v3-scheme-recursive-divide" class="block truncate text-sm text-slate-600 hover:text-slate-900">v3: Scheme: Recursive Divide</a></li><li class="pl-4"><a href="#v4-strategy-accelerate-the-divide-pattern" class="block truncate text-sm text-slate-600 hover:text-slate-900">v4: Strategy: Accelerate the divide pattern</a></li><li class="pl-4"><a href="#v4-plus-tune-experiment-on-tile_size" class="block truncate text-sm text-slate-600 hover:text-slate-900">v4-plus: Tune: Experiment on TILE_SIZE</a></li><li class="pl-4"><a href="#v5-strategy-avx2" class="block truncate text-sm text-slate-600 hover:text-slate-900">v5: Strategy: AVX2</a></li><li class="pl-4"><a href="#v6-strategy-avx2---enhanced-version" class="block truncate text-sm text-slate-600 hover:text-slate-900">v6: Strategy: AVX2 - enhanced version</a></li><li class="pl-0"><a href="#conclusion--reflection" class="block truncate text-sm text-slate-600 hover:text-slate-900">Conclusion &amp; Reflection</a></li><li class="pl-0"><a href="#reference" class="block truncate text-sm text-slate-600 hover:text-slate-900">Reference</a></li><li class="pl-4"><a href="#footnote-label" class="block truncate text-sm text-slate-600 hover:text-slate-900">Footnotes</a></li></ul></nav></aside><script>
      // 元素
      const btn = document.querySelector('[data-toc-toggle]');
      const icon = btn?.querySelector('[data-toc-icon]');
      const drawer = document.querySelector('[data-toc-drawer]');
      const overlay = document.querySelector('[data-toc-overlay]');
      const links = drawer?.querySelectorAll('a[href^="#"]');
      const idToLink = new Map();
      links?.forEach(a => idToLink.set(a.getAttribute('href')?.slice(1), a));

      // 开关（并同步箭头方向）
      function openTOC(){
        drawer?.classList.remove('-translate-x-full');
        overlay?.classList.remove('hidden');
        btn?.setAttribute('aria-expanded','true');
        icon?.classList.add('rotate-180'); // → 变 ←
      }
      function closeTOC(){
        drawer?.classList.add('-translate-x-full');
        overlay?.classList.add('hidden');
        btn?.setAttribute('aria-expanded','false');
        icon?.classList.remove('rotate-180'); // 还原 →
      }

      btn?.addEventListener('click', () => {
        if (drawer?.classList.contains('-translate-x-full')) {
          openTOC(); 
        } 
        else closeTOC();
      });
      overlay?.addEventListener('click', closeTOC);
      document.addEventListener('keydown', (e) => { if (e.key === 'Escape') closeTOC(); });

      const observer = new IntersectionObserver((entries) => {
        entries.forEach((entry) => {
          const id = entry.target.getAttribute('id');
          const link = id && idToLink.get(id);
          if (!link) return;
          if (entry.isIntersecting) {
            links?.forEach((l) => l.classList.remove('text-slate-900','font-medium'));
            link.classList.add('text-slate-900','font-medium');
          }
        });
      }, { rootMargin: '0px 0px -70% 0px', threshold: 0.1 });

      document.querySelectorAll('article :is(h1,h2,h3)').forEach((h) => observer.observe(h));
    </script>  <div class="mx-auto w-full max-w-5xl"> <!-- 长文阅读卡片（整块内容隔离背景） --> <div class="read-card"> <nav class="text-sm text-slate-500 mb-4" data-breadcrumb> <a class="hover:underline" href="/">首页</a> <span class="mx-1">/</span> <a class="hover:underline" href="/archive/">Cacheline(Archive)</a> </nav> <header class="not-prose mb-6 text-center"> <h1 class="text-3xl md:text-4xl font-semibold leading-tight tracking-tight !max-w-none [text-wrap:balance]"> Cache Oblivious GEMM Implementation </h1> <div class="mt-2 flex flex-wrap items-center justify-center gap-3 text-sm text-slate-500"> <span class="inline-flex items-center">
更新于 2025/9/19  </span>  <div class="inline-flex items-center"> <div class="flex flex-wrap items-center gap-2"><a href="https://github.com/l1cacheDell/cache-oblivious-gemm" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1 rounded-full border px-3 py-1.5 text-sm
                  hover:shadow-sm transition bg-white text-slate-700" aria-label="Code: COGEMM"><span class="inline-flex items-center justify-center size-5 rounded-full bg-black/5"><iconify-icon icon="mdi:github" class="text-base text-black"></iconify-icon></span><span class="whitespace-nowrap">COGEMM</span></a></div> </div> </div> </header> <article class="prose prose-slate prose-wide">  <h1 id="pre-requirement">Pre-requirement<a class="anchor" href="#pre-requirement"><span class="icon icon-link"></span></a></h1>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme="github-light"><code data-language="bash" data-theme="github-light" style="display: grid;"><span data-line=""><span style="color:#6A737D"># general preparation</span></span>
<span data-line=""><span style="color:#6F42C1">python</span><span style="color:#005CC5"> -m</span><span style="color:#032F62"> pip</span><span style="color:#032F62"> install</span><span style="color:#005CC5"> -U</span><span style="color:#032F62"> pip</span><span style="color:#032F62"> setuptools</span><span style="color:#032F62"> wheel</span></span>
<span data-line=""><span style="color:#6F42C1">python</span><span style="color:#005CC5"> -m</span><span style="color:#032F62"> pip</span><span style="color:#032F62"> install</span><span style="color:#005CC5"> -U</span><span style="color:#032F62"> pybind11</span><span style="color:#032F62"> numpy</span></span>
<span data-line=""><span style="color:#6F42C1">sudo</span><span style="color:#032F62"> apt</span><span style="color:#032F62"> install</span><span style="color:#005CC5"> -y</span><span style="color:#032F62"> build-essential</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># install package</span></span>
<span data-line=""><span style="color:#6F42C1">python</span><span style="color:#032F62"> setup.py</span><span style="color:#032F62"> install</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D"># run the test script</span></span>
<span data-line=""><span style="color:#6F42C1">python</span><span style="color:#032F62"> main.py</span></span></code></pre></figure>
<p>My environment information:</p>
<ul>
<li>OS: WSL2</li>
<li>CPU: Intel Core i7-13700H
<ul>
<li>L1d: 480 KiB (10 instances)</li>
<li>L1i: 320 KiB (10 instances)</li>
<li>L2: 12.5 MiB (10 instances)</li>
<li>L3: 24 MiB (1 instance)</li>
</ul>
</li>
<li>Memory: 16GB</li>
</ul>
<h1 id="cache-oblivious-gemm-implementation-report">Cache-oblivious GEMM Implementation Report<a class="anchor" href="#cache-oblivious-gemm-implementation-report"><span class="icon icon-link"></span></a></h1>
<p>To implement and see what’s the difference between the different method, I decided to set up the environment as C++ implementation and python invokation as well as benchmarking. So we need to install pybind11 and numpy and matplotlib before our experiment.</p>
<p>The data type we choose for this experiment is <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>float32</span></span></code></span>.</p>
<p>To be aware of different optimizations’ effect on GEMM, I decided to arrange and extend this report in a step-by-step order, which is also a common and straight-forward way followed by all high-performance computing programmer.</p>
<h2 id="v1-naive-gemm">v1: naive GEMM<a class="anchor" href="#v1-naive-gemm"><span class="icon icon-link"></span></a></h2>
<p>The very first thing is to implement a naive version, with nearly no optimization:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="cpp" data-theme="github-light"><code data-language="cpp" data-theme="github-light" style="display: grid;"><span data-line=""><span style="color:#D73A49">#define</span><span style="color:#6F42C1"> OFFSET</span><span style="color:#24292E">(</span><span style="color:#E36209">i</span><span style="color:#24292E">, </span><span style="color:#E36209">j</span><span style="color:#24292E">, </span><span style="color:#E36209">ld</span><span style="color:#24292E">) ((i </span><span style="color:#D73A49">*</span><span style="color:#24292E"> ld) </span><span style="color:#D73A49">+</span><span style="color:#24292E"> j)</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">// single precision gemm, float32</span></span>
<span data-line=""><span style="color:#D73A49">void</span><span style="color:#6F42C1"> sgemm_naive</span><span style="color:#24292E">(</span><span style="color:#D73A49">float*</span><span style="color:#E36209"> a</span><span style="color:#24292E">, </span><span style="color:#D73A49">float*</span><span style="color:#E36209"> b</span><span style="color:#24292E">, </span><span style="color:#D73A49">float*</span><span style="color:#E36209"> c</span><span style="color:#24292E">, </span><span style="color:#D73A49">const</span><span style="color:#D73A49"> size_t</span><span style="color:#E36209"> M</span><span style="color:#24292E">, </span><span style="color:#D73A49">const</span><span style="color:#D73A49"> size_t</span><span style="color:#E36209"> N</span><span style="color:#24292E">, </span><span style="color:#D73A49">const</span><span style="color:#D73A49"> size_t</span><span style="color:#E36209"> K</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#D73A49">    for</span><span style="color:#24292E"> (</span><span style="color:#D73A49">int</span><span style="color:#24292E"> i </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 0</span><span style="color:#24292E">; i </span><span style="color:#D73A49">&#x3C;</span><span style="color:#24292E"> M; i</span><span style="color:#D73A49">++</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#D73A49">        for</span><span style="color:#24292E"> (</span><span style="color:#D73A49">int</span><span style="color:#24292E"> j </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 0</span><span style="color:#24292E">; j </span><span style="color:#D73A49">&#x3C;</span><span style="color:#24292E"> N; j</span><span style="color:#D73A49">++</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#D73A49">            for</span><span style="color:#24292E"> (</span><span style="color:#D73A49">int</span><span style="color:#24292E"> k </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 0</span><span style="color:#24292E">; k </span><span style="color:#D73A49">&#x3C;</span><span style="color:#24292E"> K; k</span><span style="color:#D73A49">++</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#24292E">                c[</span><span style="color:#6F42C1">OFFSET</span><span style="color:#24292E">(i, j, N)] </span><span style="color:#D73A49">+=</span><span style="color:#24292E"> a[</span><span style="color:#6F42C1">OFFSET</span><span style="color:#24292E">(i, k, K)] </span><span style="color:#D73A49">*</span><span style="color:#24292E"> b[</span><span style="color:#6F42C1">OFFSET</span><span style="color:#24292E">(k, j, N)];</span></span>
<span data-line=""><span style="color:#24292E">            }</span></span>
<span data-line=""><span style="color:#24292E">        }</span></span>
<span data-line=""><span style="color:#24292E">    }</span></span>
<span data-line=""><span style="color:#24292E">}</span></span></code></pre></figure>
<p>Our testing matrix sizes ranges from 32 to 1024, (from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">2^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">2^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span>), 6 groups in total.</p>
<p>To benchmark the performance, we need to adopt the standarized symbol: <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>GFLOPS</span></span></code></span> to measure our program, this is a metric we would have seen in multiple libraries of high performance computing.</p>
<p>The total float operation of GEMM could be computed as: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>L</mi><mi>O</mi><mi>P</mi><mi>s</mi><mo>≈</mo><mn>2</mn><mo>×</mo><mi>M</mi><mo>×</mo><mi>N</mi><mo>×</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">FLOPs \approx 2 \times M \times N \times K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em;">OP</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>, and GFLOPS could also be computed as:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>G</mi><mi>F</mi><mi>L</mi><mi>O</mi><mi>P</mi><mi>S</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>L</mi><mi>O</mi><mi>P</mi><mi>S</mi></mrow><mrow><msup><mn>10</mn><mn>9</mn></msup><mo>×</mo><mi>r</mi><mi>u</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">GFLOPS = \frac{FLOPS}{10^9 \times runtime}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">GF</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.05764em;">OPS</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1297em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord mathnormal">e</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.05764em;">OPS</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>Based on this simple and efficient metric setting, we could easily draw a figure like this:</p>
<p><img src="/assets/Report_cache_oblivious_gemm_DONG_YAZHU/image-20250919002257347.png" alt="image-20250919002257347"></p>
<p>It’s pretty plain, and as I have expected and observed in the past CPU GEMM implementations. The naive GEMM implementation got 2.90 GFLOPS at best case when <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">MNK=64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span></span></span></span>. As I could infer, this could be the best size for trading off the matrix block size, and the cache size.</p>
<blockquote>
<p>Note in advance that, the reason why the y-axis was set at range of 0 to 60 is that, I used to implement a certain version of cache-aware GEMM, and reached the GFLOPS=55.8, so I knew that <em>60</em> would be the maximum value.</p>
</blockquote>
<h2 id="v2-strategy-multi-threading">v2: Strategy: Multi-Threading<a class="anchor" href="#v2-strategy-multi-threading"><span class="icon icon-link"></span></a></h2>
<p>Maybe this strategy is applied too early, but I still prefer to adopt this optimization at the very first time.</p>
<p>The reason why using multi-threading strategy could be listed as follows:</p>
<ol>
<li>Each thread is only responsible for one block of result matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>. For a 512 x 512 matrix, assuming we have 8 threads, each thread would compute a 128 x 256 block of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>, and all 8 threads compute 8 blocks, which will finally compose a complete matrix. What is the most important is that, threads could compute in parallel, which reduces the runtime.</li>
<li>When it comes to cache, (L1, L2 cache), it is owned by physical CPU core. By applying multi-thread strategy, we bind each thread to a CPU core, which is making best use of its own cache, as well as <strong>setting up a good environment for us to optimize the cache access pattern in the later part.</strong> So I choose to apply this strategy first.</li>
</ol>
<p>There is one concern, that modern CPUs are built with <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>SMT</span></span></code></span> technique, which means one physical CPU core could handle two simultaneous threads, if we have 8 CPU cores, should we launch 16 threads?</p>
<p>My previous CPU-GEMM optimization experience proved that, it would be better to <strong>keep the same amount of threads as physical CPU cores</strong>, the reason is that, although two threads could compute in parallel, they could also compete for the L1 and L2 cache space, too. In order to avoid such “cache race”, keeping it simple could be the better choice.</p>
<p>V2 code could be as follow:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="cpp" data-theme="github-light"><code data-language="cpp" data-theme="github-light" style="display: grid;"><span data-line=""><span style="color:#D73A49">template</span><span style="color:#24292E"> &#x3C;</span><span style="color:#D73A49">const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> NUM_THREADS </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 8</span><span style="color:#6A737D"> /* The device may have more than 8 cores, but to align with matrix size, use 8 cores to compte*/</span><span style="color:#24292E">></span></span>
<span data-line=""><span style="color:#D73A49">void</span><span style="color:#6F42C1"> sgemm_v2</span><span style="color:#24292E">(</span><span style="color:#D73A49">float*</span><span style="color:#E36209"> a</span><span style="color:#24292E">, </span><span style="color:#D73A49">float*</span><span style="color:#E36209"> b</span><span style="color:#24292E">, </span><span style="color:#D73A49">float*</span><span style="color:#E36209"> c</span><span style="color:#24292E">, </span><span style="color:#D73A49">const</span><span style="color:#D73A49"> size_t</span><span style="color:#E36209"> M</span><span style="color:#24292E">, </span><span style="color:#D73A49">const</span><span style="color:#D73A49"> size_t</span><span style="color:#E36209"> N</span><span style="color:#24292E">, </span><span style="color:#D73A49">const</span><span style="color:#D73A49"> size_t</span><span style="color:#E36209"> K</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#6A737D">    // use multi thread to handle this GEMM process</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> TILE_SIZE_X </span><span style="color:#D73A49">=</span><span style="color:#24292E"> M </span><span style="color:#D73A49">/</span><span style="color:#24292E"> (NUM_THREADS </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">);</span><span style="color:#6A737D">  // ------------ > x, the horizon</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> TILE_SIZE_Y </span><span style="color:#D73A49">=</span><span style="color:#24292E"> N </span><span style="color:#D73A49">/</span><span style="color:#24292E"> (</span><span style="color:#005CC5">2</span><span style="color:#24292E">);</span><span style="color:#6A737D">                // y, the vertical</span></span>
<span data-line=""><span style="color:#D73A49">    #pragma</span><span style="color:#6F42C1"> omp</span><span style="color:#6F42C1"> parallel</span><span style="color:#6F42C1"> num_threads</span><span style="color:#24292E">(</span><span style="color:#6F42C1">NUM_THREADS</span><span style="color:#24292E">)</span></span>
<span data-line=""><span style="color:#24292E">    {</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> total_threads </span><span style="color:#D73A49">=</span><span style="color:#6F42C1"> omp_get_num_threads</span><span style="color:#24292E">();</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> tid </span><span style="color:#D73A49">=</span><span style="color:#6F42C1"> omp_get_thread_num</span><span style="color:#24292E">();</span><span style="color:#6A737D">       // current tid, same programming model as cuda</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> tile_id_x </span><span style="color:#D73A49">=</span><span style="color:#24292E"> tid </span><span style="color:#D73A49">%</span><span style="color:#24292E"> (NUM_THREADS </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">);</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> tile_id_y </span><span style="color:#D73A49">=</span><span style="color:#24292E"> tid </span><span style="color:#D73A49">/</span><span style="color:#24292E"> (NUM_THREADS </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">);</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> block_x </span><span style="color:#D73A49">=</span><span style="color:#24292E"> tile_id_x </span><span style="color:#D73A49">*</span><span style="color:#24292E"> TILE_SIZE_X;</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> block_y </span><span style="color:#D73A49">=</span><span style="color:#24292E"> tile_id_y </span><span style="color:#D73A49">*</span><span style="color:#24292E"> TILE_SIZE_Y;</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">        // compute</span></span>
<span data-line=""><span style="color:#D73A49">        for</span><span style="color:#24292E"> (</span><span style="color:#D73A49">int</span><span style="color:#24292E"> m </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 0</span><span style="color:#24292E">; m </span><span style="color:#D73A49">&#x3C;</span><span style="color:#24292E"> TILE_SIZE_Y; m</span><span style="color:#D73A49">++</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#D73A49">            int</span><span style="color:#24292E"> start_idx_m </span><span style="color:#D73A49">=</span><span style="color:#24292E"> block_y </span><span style="color:#D73A49">+</span><span style="color:#24292E"> m;</span></span>
<span data-line=""><span style="color:#D73A49">            for</span><span style="color:#24292E"> (</span><span style="color:#D73A49">int</span><span style="color:#24292E"> n </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 0</span><span style="color:#24292E">; n </span><span style="color:#D73A49">&#x3C;</span><span style="color:#24292E"> TILE_SIZE_X; n</span><span style="color:#D73A49">++</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#D73A49">                int</span><span style="color:#24292E"> start_idx_n </span><span style="color:#D73A49">=</span><span style="color:#24292E"> block_x </span><span style="color:#D73A49">+</span><span style="color:#24292E"> n;</span></span>
<span data-line=""><span style="color:#D73A49">                for</span><span style="color:#24292E"> (</span><span style="color:#D73A49">int</span><span style="color:#24292E"> k </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 0</span><span style="color:#24292E">; k </span><span style="color:#D73A49">&#x3C;</span><span style="color:#24292E"> M; k</span><span style="color:#D73A49">++</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#24292E">                    c[</span><span style="color:#6F42C1">OFFSET</span><span style="color:#24292E">(start_idx_m, start_idx_n, N)] </span><span style="color:#D73A49">+=</span><span style="color:#24292E"> a[</span><span style="color:#6F42C1">OFFSET</span><span style="color:#24292E">(start_idx_m, k, K)] </span><span style="color:#D73A49">*</span><span style="color:#24292E"> b[</span><span style="color:#6F42C1">OFFSET</span><span style="color:#24292E">(k, start_idx_n, N)];</span></span>
<span data-line=""><span style="color:#24292E">                }</span></span>
<span data-line=""><span style="color:#24292E">            }</span></span>
<span data-line=""><span style="color:#24292E">        }</span></span>
<span data-line=""><span style="color:#24292E">    }</span></span>
<span data-line=""><span style="color:#24292E">}</span></span></code></pre></figure>
<p>Here I chose to use <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>OpenMP</span></span></code></span> as the parallel computation library, because launching threads manually will lead to extra overhead, comparing to use one highly-optimized library. Meanwhile, to keep code clean and simple, as well as focusing on the topic <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>cache-oblivious GEMM</span></span></code></span>, I would like to save the strength on such code style discussion.</p>
<p>Each thread will compute its starting index of matrix block, and computes in parallel, the benchmark:</p>
<p><img src="/assets/Report_cache_oblivious_gemm_DONG_YAZHU/image-20250919004714331.png" alt="image-20250919004714331"></p>
<p>The highest GFLOPS is 9.58 at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">MNK=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span>, then drops to 5.00 at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">MNK=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1024</span></span></span></span>, the speedup compared to naive group is nearly <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">5\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mord">×</span></span></span></span>, except the first group. I think that when MNK size is small, the multi-thread’s launching and collecting cost would dominate the process, thus v2 is slower than v1 at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">MNK=32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">32</span></span></span></span>.</p>
<h2 id="v3-scheme-recursive-divide">v3: Scheme: Recursive Divide<a class="anchor" href="#v3-scheme-recursive-divide"><span class="icon icon-link"></span></a></h2>
<p>The title starts with <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>scheme</span></span></code></span>, instead of <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>strategy</span></span></code></span>, because our requirement is to implement a recursive divide and conquer GEMM. So this step is to meet the basic requirement, <em>not an optimization attempt.</em></p>
<p>Now we will discuss how to implement a cache-oblivious GEMM. My attempt is to divide the matrix into 4 blocks each time, recursively, untill the block size is less than a certain threshold.</p>
<p>Assuming the matrix size is 512 x 512, after our threading strategy, each thread would compute a 256 x 128 block. This is the start point for our recursive division applying. For a 256 x 128 block, define a threshold size 64, the divide steps could be described as follows:</p>
<ol>
<li>1st division, 256 x 128 → [128 x 64] x 4</li>
<li>2st division, 128 x 64 → [64 x 32] x 4</li>
</ol>
<p>Now both <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> have been limited to our setting threshold, we would develop a micro kernel, to compute a small block’s GEMM.</p>
<p>Note that, there are a lot of variables to be created for computing the index, which is confused and puzzling. To avoid this I would like to clarify here:</p>
<ul>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>C_M, C_N, C_K</span></span></code></span> is the abbreviation of <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>current M, current N, current K</span></span></code></span> size, [64, 32] e.g.</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>lda, ldb, ldc</span></span></code></span> is the leading dimension of matrix. Although we are computing a tiny block, in order to get the correct index we will fetch data across rows and columns, thus leading dimension should be clarified, and distinguished from <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>current matrix size</span></span></code></span>. 512 e.g.</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>global_start_x, global_start_y</span></span></code></span> is the global coordination of our starting point, as we have mentioned that we are doing multi-thread computation, each thread would have a starting point to figure out the exact location.</li>
</ul>
<p>Considering the tight textual space of this report, I will not represent the total v3 code, but I would present only the recursive method:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="cpp" data-theme="github-light"><code data-language="cpp" data-theme="github-light" style="display: grid;"><span data-line=""><span style="color:#6A737D">// the input a,b,c will be added the thread block offset before passing in.</span></span>
<span data-line=""><span style="color:#D73A49">template</span><span style="color:#24292E"> &#x3C;</span><span style="color:#D73A49">const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> TILE_SIZE </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 64</span><span style="color:#24292E">></span></span>
<span data-line=""><span style="color:#D73A49">void</span><span style="color:#6F42C1"> sgemm_v3_recursive</span><span style="color:#24292E">(</span><span style="color:#D73A49">float*</span><span style="color:#E36209"> a</span><span style="color:#24292E">, </span><span style="color:#D73A49">float*</span><span style="color:#E36209"> b</span><span style="color:#24292E">, </span><span style="color:#D73A49">float*</span><span style="color:#E36209"> c</span><span style="color:#24292E">, </span></span>
<span data-line=""><span style="color:#D73A49">                            size_t</span><span style="color:#E36209"> C_M</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> C_N</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> C_K</span><span style="color:#24292E">, </span></span>
<span data-line=""><span style="color:#D73A49">                            size_t</span><span style="color:#E36209"> lda</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> ldb</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> ldc</span><span style="color:#24292E">) {</span></span>
<span data-line=""><span style="color:#D73A49">    if</span><span style="color:#24292E"> (C_M </span><span style="color:#D73A49">&#x3C;=</span><span style="color:#24292E"> TILE_SIZE </span><span style="color:#D73A49">&#x26;&#x26;</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">&#x3C;=</span><span style="color:#24292E"> TILE_SIZE) {</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_v3_micro_kernel</span><span style="color:#24292E">(a, b, c, C_M, C_N, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#D73A49">        return</span><span style="color:#24292E">;</span></span>
<span data-line=""><span style="color:#24292E">    }</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    // divide recursively</span></span>
<span data-line=""><span style="color:#6A737D">    // assume we divide the matrix 2x2</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> M_u </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_M </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">;</span><span style="color:#6A737D">     // `u` stands for up</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> M_d </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_M </span><span style="color:#D73A49">-</span><span style="color:#24292E"> M_u;</span><span style="color:#6A737D">   // `d` stands for down</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> N_l </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">;</span><span style="color:#6A737D">     // `l` stands for left</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> N_r </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">-</span><span style="color:#24292E"> N_l;</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6F42C1">    sgemm_v3_recursive</span><span style="color:#24292E">(a,               b,              c,                      M_u, N_l, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#6F42C1">    sgemm_v3_recursive</span><span style="color:#24292E">(a </span><span style="color:#D73A49">+</span><span style="color:#24292E"> M_u </span><span style="color:#D73A49">*</span><span style="color:#24292E"> lda, b,            c </span><span style="color:#D73A49">+</span><span style="color:#24292E"> M_u </span><span style="color:#D73A49">*</span><span style="color:#24292E"> ldc,          M_d, N_l, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#6F42C1">    sgemm_v3_recursive</span><span style="color:#24292E">(a,               b </span><span style="color:#D73A49">+</span><span style="color:#24292E"> N_l,    c </span><span style="color:#D73A49">+</span><span style="color:#24292E"> N_l,                M_u, N_r, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#6F42C1">    sgemm_v3_recursive</span><span style="color:#24292E">(a </span><span style="color:#D73A49">+</span><span style="color:#24292E"> M_u </span><span style="color:#D73A49">*</span><span style="color:#24292E"> lda, b </span><span style="color:#D73A49">+</span><span style="color:#24292E"> N_l,      c </span><span style="color:#D73A49">+</span><span style="color:#24292E"> M_u </span><span style="color:#D73A49">*</span><span style="color:#24292E"> ldc </span><span style="color:#D73A49">+</span><span style="color:#24292E"> N_l, M_d, N_r, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#24292E">}</span></span></code></pre></figure>
<p>Each level divide the matrix block to 4 smaller blocks, recursively, the benchmark:</p>
<p><img src="/assets/Report_cache_oblivious_gemm_DONG_YAZHU/image-20250919112348439.png" alt="image-20250919112348439"></p>
<p>Obviously, the result did not meet our expectation. A smaller block size will lead to better and more friendly cache access pattern, boosting performance improvement, but the result we observed contradicts with this prior experience.</p>
<p>To address this issue, let’s review the potential improvement points:</p>
<ol>
<li>
<p>Why the recursive way is 2 x 2? Any existing previous research demonstrates that this is the best recursive way?</p>
</li>
<li>
<p>Why the threshold is <em>64</em>? Any existing blogs or paper demonstrates this value’s effectiveness?</p>
</li>
<li>
<p>Why inner product? The outer product could be better.</p>
</li>
<li>
<p>Although we have divided the blocks, we only divide the result matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>, that is to say, our computation way of smallest matrix block could also be described as:</p>
<ol>
<li>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="cpp" data-theme="github-light"><code data-language="cpp" data-theme="github-light" style="display: grid;"><span data-line=""><span style="color:#24292E">a block: [</span><span style="color:#E36209">tile_size</span><span style="color:#24292E">, </span><span style="color:#E36209">total_K</span><span style="color:#24292E">] @ b block: [total_K, tile_size] = c block: [tile_size, tile_size]</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#24292E">e.g.</span></span>
<span data-line=""><span style="color:#24292E">a block: [64, 512] @ b block: [512, 64] = c block: [64, 64]</span></span></code></pre></figure>
</li>
<li>
<p>which means: I am still accessing the long-wide total range, not matter in a, b or c, which strongly against our initial intention of recursive GEMM.</p>
</li>
<li>
<p>Alternatively, I should find a way that only computes one small block each time, like a [8x8] @ [8x8], instead of computing the whole range.</p>
</li>
</ol>
</li>
</ol>
<h2 id="v4-strategy-accelerate-the-divide-pattern">v4: Strategy: Accelerate the divide pattern<a class="anchor" href="#v4-strategy-accelerate-the-divide-pattern"><span class="icon icon-link"></span></a></h2>
<p>After sorting out all potential ways, I decided to improve the cache-oblivious recursive GEMM from following aspects:</p>
<ol>
<li>
<p><strong>The division strategy</strong>: the previous division is rough, and only divides <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> into blocks, in order to reach the real small-block GEMM, I modified the division logic:</p>
<ol>
<li>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="cpp" data-theme="github-light"><code data-language="cpp" data-theme="github-light" style="display: grid;"><span data-line=""><span style="color:#D73A49">template</span><span style="color:#24292E"> &#x3C;</span><span style="color:#D73A49">const</span><span style="color:#D73A49"> int</span><span style="color:#24292E"> BASE </span><span style="color:#D73A49">=</span><span style="color:#005CC5"> 64</span><span style="color:#6A737D"> /* threshold */</span><span style="color:#24292E">></span></span>
<span data-line=""><span style="color:#D73A49">void</span><span style="color:#6F42C1"> sgemm_v4_recursive</span><span style="color:#24292E">(</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> float*</span><span style="color:#6F42C1"> __restrict</span><span style="color:#E36209"> A</span><span style="color:#24292E">,</span></span>
<span data-line=""><span style="color:#D73A49">    const</span><span style="color:#D73A49"> float*</span><span style="color:#6F42C1"> __restrict</span><span style="color:#E36209"> B</span><span style="color:#24292E">,</span></span>
<span data-line=""><span style="color:#D73A49">    float*</span><span style="color:#6F42C1"> __restrict</span><span style="color:#E36209">       C</span><span style="color:#24292E">,</span></span>
<span data-line=""><span style="color:#D73A49">    size_t</span><span style="color:#E36209"> C_M</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> C_N</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> C_K</span><span style="color:#24292E">,</span></span>
<span data-line=""><span style="color:#D73A49">    size_t</span><span style="color:#E36209"> lda</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> ldb</span><span style="color:#24292E">, </span><span style="color:#D73A49">size_t</span><span style="color:#E36209"> ldc</span><span style="color:#24292E">)</span></span>
<span data-line=""><span style="color:#24292E">{</span></span>
<span data-line=""><span style="color:#D73A49">    if</span><span style="color:#24292E"> (C_M </span><span style="color:#D73A49">&#x3C;=</span><span style="color:#24292E"> (</span><span style="color:#D73A49">size_t</span><span style="color:#24292E">)BASE </span><span style="color:#D73A49">&#x26;&#x26;</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">&#x3C;=</span><span style="color:#24292E"> (</span><span style="color:#D73A49">size_t</span><span style="color:#24292E">)BASE </span><span style="color:#D73A49">&#x26;&#x26;</span><span style="color:#24292E"> C_K </span><span style="color:#D73A49">&#x3C;=</span><span style="color:#24292E"> (</span><span style="color:#D73A49">size_t</span><span style="color:#24292E">)BASE) {</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_micro_v4_blockbuffer_kernel</span><span style="color:#24292E">&#x3C;</span><span style="color:#6F42C1">BASE</span><span style="color:#24292E">>(A, B, C, C_M, C_N, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#D73A49">        return</span><span style="color:#24292E">;</span></span>
<span data-line=""><span style="color:#24292E">    }</span></span>
<span data-line=""> </span>
<span data-line=""><span style="color:#6A737D">    // split recursively along the longest dim</span></span>
<span data-line=""><span style="color:#D73A49">    if</span><span style="color:#24292E"> (C_M </span><span style="color:#D73A49">>=</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">&#x26;&#x26;</span><span style="color:#24292E"> C_M </span><span style="color:#D73A49">>=</span><span style="color:#24292E"> C_K) {</span></span>
<span data-line=""><span style="color:#6A737D">        // split M</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> M1 </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_M </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">;</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> M2 </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_M </span><span style="color:#D73A49">-</span><span style="color:#24292E"> M1;</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_v4_recursive</span><span style="color:#24292E">&#x3C;</span><span style="color:#6F42C1">BASE</span><span style="color:#24292E">>(A,              B, C,              M1, C_N, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_v4_recursive</span><span style="color:#24292E">&#x3C;</span><span style="color:#6F42C1">BASE</span><span style="color:#24292E">>(A </span><span style="color:#D73A49">+</span><span style="color:#24292E"> M1 </span><span style="color:#D73A49">*</span><span style="color:#24292E"> lda,   B, C </span><span style="color:#D73A49">+</span><span style="color:#24292E"> M1 </span><span style="color:#D73A49">*</span><span style="color:#24292E"> ldc,   M2, C_N, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#24292E">    } </span><span style="color:#D73A49">else</span><span style="color:#D73A49"> if</span><span style="color:#24292E"> (C_N </span><span style="color:#D73A49">>=</span><span style="color:#24292E"> C_M </span><span style="color:#D73A49">&#x26;&#x26;</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">>=</span><span style="color:#24292E"> C_K) {</span></span>
<span data-line=""><span style="color:#6A737D">        // split N</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> N1 </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">;</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> N2 </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_N </span><span style="color:#D73A49">-</span><span style="color:#24292E"> N1;</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_v4_recursive</span><span style="color:#24292E">&#x3C;</span><span style="color:#6F42C1">BASE</span><span style="color:#24292E">>(A, B,             C,             C_M, N1, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_v4_recursive</span><span style="color:#24292E">&#x3C;</span><span style="color:#6F42C1">BASE</span><span style="color:#24292E">>(A, B </span><span style="color:#D73A49">+</span><span style="color:#24292E"> N1,        C </span><span style="color:#D73A49">+</span><span style="color:#24292E"> N1,        C_M, N2, C_K, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#24292E">    } </span><span style="color:#D73A49">else</span><span style="color:#24292E"> {</span></span>
<span data-line=""><span style="color:#6A737D">        // split K: this is outer product!</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> K1 </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_K </span><span style="color:#D73A49">/</span><span style="color:#005CC5"> 2</span><span style="color:#24292E">;</span></span>
<span data-line=""><span style="color:#D73A49">        const</span><span style="color:#D73A49"> size_t</span><span style="color:#24292E"> K2 </span><span style="color:#D73A49">=</span><span style="color:#24292E"> C_K </span><span style="color:#D73A49">-</span><span style="color:#24292E"> K1;</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_v4_recursive</span><span style="color:#24292E">&#x3C;</span><span style="color:#6F42C1">BASE</span><span style="color:#24292E">>(A,           B,             C, C_M, C_N, K1, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#6F42C1">        sgemm_v4_recursive</span><span style="color:#24292E">&#x3C;</span><span style="color:#6F42C1">BASE</span><span style="color:#24292E">>(A </span><span style="color:#D73A49">+</span><span style="color:#24292E"> K1,      B </span><span style="color:#D73A49">+</span><span style="color:#24292E"> K1 </span><span style="color:#D73A49">*</span><span style="color:#24292E"> ldb,  C, C_M, C_N, K2, lda, ldb, ldc);</span></span>
<span data-line=""><span style="color:#24292E">    }</span></span>
<span data-line=""><span style="color:#24292E">}</span></span></code></pre></figure>
</li>
<li>
<p>At this time, we split <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo separator="true">,</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A, B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> matrix into smaller blocks, and operates real small-block GEMM, to make our GEMM cache harmonious.</p>
</li>
<li>
<p>Meanwhile, our recursively splitting strategy is half-largest-split, each time we will split the largest dimension to two halves. This strategy ensures that we could get a pretty-resized small matrix block, instead of getting some long-ranged sub-matrix.</p>
</li>
</ol>
</li>
<li>
<p><strong>Kernel Optimization</strong>: Our kernel is still a naive one, no buffering, cross row accessing, etc. I decided to optimize our kernel in following aspects:</p>
<ol>
<li>Accessing A, B, and C matrix block in row, instead of cross-row accessing. This will keep data linear in cache, and accessing them continuously. Of course this requires some re-order of computating method.</li>
<li>Re-order the triple loop. As we all know, GEMM requires <em>at least</em> three loop: <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>i,j,k</span></span></code></span>, but the order of them could be re-arranged. In previous implementation I just used <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>i, j, k</span></span></code></span>, but how about <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>i,k,j</span></span></code></span> or <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>k,i,j</span></span></code></span>? I tried to find the best combination of this triple loop sequence.</li>
<li>Buffering. <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>C[i,j] += A[i,k] * B[k,j]</span></span></code></span> is naive and plain. To be more specific, this would cause <strong>one read and one write from cache each time</strong>. What about we use some variables as <strong>buffer</strong>, <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>sum</span></span></code></span>, e.g. This variable is stored in registers instead of cache or memory, which is the fastest. After finishing adding, write it back to <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>C[i,j]</span></span></code></span>, saving a lot of reading overhead.</li>
</ol>
</li>
</ol>
<p><img src="/assets/Report_cache_oblivious_gemm_DONG_YAZHU/image-20250919125747064.png" alt="image-20250919125747064"></p>
<p>It’s astonishing. I break the record of my previous best implementation’s GFLOPS (55.8, 1 year ago). The highest GFLOPS at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">MNK=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1024</span></span></span></span> is 77.55!</p>
<p>There are still some factors I need to consider:</p>
<ol>
<li>Why <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">MNK=64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span></span></span></span> GFLOPS is so high? And drops down in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>256</mn><mo separator="true">,</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">MNK=256,512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">256</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">512</span></span></span></span> ? This remains a mystery.</li>
<li>The <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span> I used is 64, what about tuning this size?</li>
<li>The kernel is not fully optimized yet (AVX2 Instruction Set) , which means our program still has optimizing space.</li>
</ol>
<h2 id="v4-plus-tune-experiment-on-tile_size">v4-plus: Tune: Experiment on <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span><a class="anchor" href="#v4-plus-tune-experiment-on-tile_size"><span class="icon icon-link"></span></a></h2>
<p>Before tuning the <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span>, I would like to put my reflection on the correctness of this approach.</p>
<p>Core issue: <strong>What we are doing is implementing a cache-oblivious recursive GEMM. Note that, it’s cache-oblivious. If we tune the <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span>, will this action contradict to our initial intention?</strong></p>
<p>No, they do not contradict to each other. Indeed we are doing cache-oblivious things, and the function does not know the exact cache-size, but the function <strong>urgently needs to know when to stop</strong>, instead of endless recursion. Besides, the <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span> would significantly affect our micro-kernel’s performace, we need to do experiments to see which size would boost our micro-kernel most.</p>
<hr>
<p>Based on such debating and reflection, let’s tune the <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span> in <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>[16, 32, 48, 64, 96, 128]</span></span></code></span>. This requires some C++ dispatch to invoke different template <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>tile_base</span></span></code></span>, which would be detailed in code implementation, not be presented in report context.</p>
<p>Different <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span>s are experimented as follows:</p>
<p><img src="/assets/Report_cache_oblivious_gemm_DONG_YAZHU/image-20250919134241609.png" alt="image-20250919134241609"></p>
<p>As shown in this figure, best performance was reached when <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_BASE=64</span></span></code></span>, and group-t64, group-t96 could nearly perform the same at large GEMM. As for small GEMM like <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>64</mn><mo separator="true">,</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">MNK=64,128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">64</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">128</span></span></span></span>, the <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span> would significantly affect the performance among which the group-t128 has best performance. The peak GFLOPS is 79.71 by group-t64 at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">MNK=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1024</span></span></span></span>.</p>
<p>In the rest optimization steps, I will use the best performance group TILE_SIZE = 64 to test the hardware limit, as we have tested out that this size could have best performance.</p>
<h2 id="v5-strategy-avx2">v5: Strategy: AVX2<a class="anchor" href="#v5-strategy-avx2"><span class="icon icon-link"></span></a></h2>
<p>We haven’t apply avx2 instruction set to our program yet. Let’s do this to our micro-kernel, which is pretty easy.</p>
<p>The avx2 implementation is plain, which could be regarded as “a translation of previous micro-kernel to 4-element continuous” versoin. The code will not be expanded in report, benchmark:</p>
<p><img src="/assets/Report_cache_oblivious_gemm_DONG_YAZHU/image-20250919143130322.png" alt="image-20250919143130322"></p>
<p>The peak GFLOPS at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">MNK=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1024</span></span></span></span> is 108.83, almost <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.36</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.36 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1.36</span><span class="mord">×</span></span></span></span> speed up compared to v4, this improvement <strong>does not meet our expectation.</strong> SIMD instructions operates 4-float-element <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>fmadd</span></span></code></span> each time. Considering the memory access, computation, and other potential affected parts, the speedup would be at least <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mord">×</span></span></span></span>.</p>
<p>In another word, current implementation performed very <strong>poorly</strong>.</p>
<p>Again, let’s review the potential optimization in our micro-kernel:</p>
<ol>
<li>Our current computing scheme is inner product, according to modern BLAS libraries, <strong>outer product</strong> could be better.</li>
<li>According to technique blogs or public discussion<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>, <strong>8x6</strong> tile size could be a better (or even best) tiny-tile for SIMD. I have known this trick, but I haven’t implemented any version yet. Let’s try at this time.
<ol>
<li>Note that: <strong>This typical tile-size requires the matrices are col-major</strong></li>
</ol>
</li>
<li>We are currently depending on <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>for</span></span></code></span> loop, this actually did a lot of thing for us, and jumped a lot of optimization chances. We need to unroll this loop manually, instead of using <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>#pragma unroll</span></span></code></span></li>
</ol>
<h2 id="v6-strategy-avx2---enhanced-version">v6: Strategy: AVX2 - enhanced version<a class="anchor" href="#v6-strategy-avx2---enhanced-version"><span class="icon icon-link"></span></a></h2>
<p>In this section, I applied all <a href="https://github.com/l1cacheDell/easy_lab/commit/f655aaece7b81591a7c995ad9f5523586197570b" rel="noopener noreferrer" target="_blank">my previous optimization</a> strategy:</p>
<ol>
<li>Converting <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo separator="true">,</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A, B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> to col-major, for <strong>outer-production.</strong></li>
<li>Inside micro-kernel, doing small <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>8x6</span></span></code></span> tile size GEMM, the total size of this micro-kernel is <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>64x64</span></span></code></span>, thus applying padding at the tail part.</li>
<li>Unroll the loop manually, to make sure variables stay in register/L1 cache/L2 cache.</li>
<li>Buffering.</li>
</ol>
<p>Detailed implementation is a bit large, thus would not be presented in our report.</p>
<p><img src="/assets/Report_cache_oblivious_gemm_DONG_YAZHU/image-20250919153427492.png" alt="image-20250919153427492"></p>
<p>In comparison, the GFLOPS peak at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">MNK=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1024</span></span></span></span> by <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>v6</span></span></code></span> is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>151.28</mn></mrow><annotation encoding="application/x-tex">151.28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">151.28</span></span></span></span>, while <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>v5</span></span></code></span> is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>119.37</mn></mrow><annotation encoding="application/x-tex">119.37</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">119.37</span></span></span></span>, and <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>v4</span></span></code></span> is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>80.75</mn></mrow><annotation encoding="application/x-tex">80.75</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">80.75</span></span></span></span>, which means v6 (<em>highly-optimized AVX2 version</em>) has almost <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mord">×</span></span></span></span> speedup compared to v4 (<em>No AVX2 versoin</em>).</p>
<p>So far, I have tried all optimization strategies I could have known and learned.</p>
<p><em>Although I have also known that <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>doing data-repacking</span></span></code></span> would help boost the performance further more, I think these are all enough.</em></p>
<h1 id="conclusion--reflection">Conclusion &#x26; Reflection<a class="anchor" href="#conclusion--reflection"><span class="icon icon-link"></span></a></h1>
<p>This is quite an unforgettable journey. Throughout this journey I got another chance to implement CPU GEMM on hand by myself again, and I got deeper unstanding on cache-oblivous method and reviewed my past implementations. In this section I would conclude our optimization strategies and answer several questions.</p>
<p>We have applied: multi-threading, recursive-divide-and-conquer, re-arranging divide pattern, tuning <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>TILE_SIZE</span></span></code></span>, using AVX2 and doing very radical optimization to AVX2 kernel, improving GFLOPS from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.88</mn></mrow><annotation encoding="application/x-tex">0.88</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.88</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>151.28</mn></mrow><annotation encoding="application/x-tex">151.28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">151.28</span></span></span></span>, which is offensively approaching the limit of our hardware. The most importantly, I beat the past myself, as well as the performance I used to reach.</p>
<p>The biggest advance, surprisingly, does not come from any my previous known strategies, but from <em>v3: recursive divide</em> to <em>v4: accelerate the divide pattern</em> (from the last figure, from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4.76</mn></mrow><annotation encoding="application/x-tex">4.76</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4.76</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>80.44</mn></mrow><annotation encoding="application/x-tex">80.44</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">80.44</span></span></span></span>, boosting <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16.89</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">16.89 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16.89</span><span class="mord">×</span></span></span></span> speedup). The latter strategies were just help improving, but never made such a break-through, <strong>which proves the effectiveness of cache-oblivious implementation and the correctness of cache-oblivious theory, again.</strong></p>
<p>Now I would like to reflect on several specific questions:</p>
<p><strong>1. Why <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>8x6</span></span></code></span> would be the best tile size?</strong></p>
<p>The word <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>best</span></span></code></span> I used in my report, is a little bit rushed. <strong>Only under the scenarios of float32 - AVX2 - col-major - outer-product GEMM</strong>, <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>8x6</span></span></code></span> could be a perfect tile size. To prove it, I would like to compute this process directly.</p>
<ul>
<li>In col-major layout of matrix, we often adopt outer-product GEMM pattern to be hardware-friendly.</li>
<li>In outerr product, we take <strong>one col from left matrix A [M, 1]</strong>. and <strong>one row from right matrix B [1, N]</strong>, to do computation, thus forming a result matrix of <strong>[M, N]</strong>, and then add to result matrix <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>.</li>
<li>At the same time, due to the col-major layout:
<ul>
<li>“one col from A” actually means a row, in physical layout of A.</li>
<li>“one row from B” actually means a col, in physical layout of B.</li>
</ul>
</li>
<li>AVX2-256bit, could exactly <strong>store 8 float32 elements</strong> in a <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>__m256</span></span></code></span> data.
<ul>
<li>which means, a <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>__m256</span></span></code></span> data could be stored in a <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm</span></span></code></span> register. (Perfectly suit-in)</li>
<li>So, we could store one physical row of A.</li>
</ul>
</li>
<li>Each time, we <strong>broadcast a scalar of B</strong>, to <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>__m256</span></span></code></span>, and do <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>_mfadd256()</span></span></code></span> with previous A data.
<ul>
<li>Note: we do not load a linear fragment of B, but <strong>broadcast a single one scalar of B instead.</strong></li>
</ul>
</li>
<li>Lastly, add the result to “one col of C”, and “one col” also means a row, in physical layout of C.</li>
</ul>
<p>Let’s consider the registers usage in this <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>8x6 trick</span></span></code></span>:</p>
<ul>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm0-ymm5</span></span></code></span> → store “6 cols of C”, with 8 elements each. That is the size of <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>8x6</span></span></code></span>.</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm6</span></span></code></span> 	      → store the temp variable of “one col of A”, one <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>__m256</span></span></code></span> data only takes one register.</li>
<li><span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm7 </span></span></code></span>           → store the temp variable of “broadcasted scalar of B”, one <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>__m256</span></span></code></span> data only takes one register.</li>
</ul>
<p>So, a <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>8x6</span></span></code></span> small tile takes 8 <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm</span></span></code></span> registers in total. Although one CPU core has 16 <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm</span></span></code></span> registers, the rest <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm</span></span></code></span> registers would be used for other purposes, such as bufferring, data-transferring, etc. Any attempt to allocate more <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm</span></span></code></span> registers would lead to performance dropping.</p>
<p>Beyond registers, the cache line could also be considered. Modern CPU’s cache line is 64B. The usage of 8 <span data-rehype-pretty-code-figure=""><code data-language="text" data-theme="github-light"><span data-line=""><span>ymm</span></span></code></span> registers take <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo><mn>256</mn><mtext>  </mtext><mi>b</mi><mi>i</mi><mi>t</mi><mi>s</mi><mo>=</mo><mn>8</mn><mo>×</mo><mn>8</mn><mtext>  </mtext><mi>B</mi><mo>=</mo><mn>64</mn><mtext>  </mtext><mi>B</mi></mrow><annotation encoding="application/x-tex">8\times256\;bits = 8 \times 8 \; B = 64\;B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">256</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">bi</span><span class="mord mathnormal">t</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>, <strong>perfectly making good use of one cache line.</strong></p>
<p><strong>2. Why performance boosts at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>N</mi><mi>K</mi><mo>=</mo><mn>64</mn><mo separator="true">,</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">MNK=64, 128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">64</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">128</span></span></span></span>? And why it drops steeply later then?</strong></p>
<p>Again, we could compute this GEMM total size:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>4</mn><mtext>  </mtext><mi>B</mi><mo>=</mo><mn>16</mn><mi>K</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">64 \times 64 \times 4 \;B= 16KB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">16</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>+</mo><mi>B</mi><mo>+</mo><mi>C</mi><mo>=</mo><mn>48</mn><mi>K</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">A+B+C=48KB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">48</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>. My working CPU has exactly <strong>48KB</strong> L1d Cache per CPU core, which means the whole GEMM operation <strong>could landing mostly in L1 Cache.</strong></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn><mo>×</mo><mn>4</mn><mtext>  </mtext><mi>B</mi><mo>=</mo><mn>64</mn><mi>K</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">128 \times 128 \times 4\;B = 64KB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">128</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">128</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">64</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>+</mo><mi>B</mi><mo>+</mo><mi>C</mi><mo>=</mo><mn>192</mn><mi>K</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">A+B+C = 192KB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">192</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>. Altough this would over-flow my L1 data cache, <strong>it could still land most in L2 Cache</strong>.</li>
</ul>
<p>Based on this observatoin, it seems explainable for the temporary peak.</p>
<p>Starting from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mi>N</mi><mo>=</mo><mi>K</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">M=N=K=256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">256</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn><mo>×</mo><mn>256</mn><mo>×</mo><mn>4</mn><mtext>  </mtext><mi>B</mi><mo>=</mo><mn>256</mn><mi>K</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">256 \times 256 \times 4\;B = 256 KB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">256</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">256</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">256</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>+</mo><mi>B</mi><mo>+</mo><mi>C</mi><mo>=</mo><mn>768</mn><mi>K</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">A+B+C=768KB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">768</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>, which exceeds the L2 cache size. The reuse of cache would be in a bad situation, and even worse if we are not doing re-packing of data. This could explain the steep dropping in the figure.</p>
<h1 id="reference">Reference<a class="anchor" href="#reference"><span class="icon icon-link"></span></a></h1>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes<a class="anchor" href="#footnote-label"><span class="icon icon-link"></span></a></h2>
<ol>
<li id="user-content-fn-1">
<p>Why 8x6 tile size is better: <a href="https://github.com/bluss/matrixmultiply/issues/34#issue-386834699" rel="noopener noreferrer" target="_blank">https://github.com/bluss/matrixmultiply/issues/34#issue-386834699</a> <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>  </article> </div> </div> <section class="mt-12" aria-label="Comments"> <script src="https://utteranc.es/client.js" repo="l1cacheDell/l1cacheDell.github.io" issue-term="title" * 'pathname' | 'title' | 任意字符串键 * label="comment" theme="github-light" crossorigin="anonymous" async></script> <noscript>请启用 JavaScript 以查看评论（Utterances）。</noscript> </section>  <footer class="border-t"> <div class="mx-auto max-w-5xl px-4 py-8 text-center text-sm text-slate-500">
© 2021-2025 L1 Cached Papers: Only the Papers That Matter — HPC, LLM Inference, Crypto.
</div> </footer> </body></html>